{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BindingMOAD  bioml  final  move.py  PDBbind  processed\traw  separated\n"
     ]
    }
   ],
   "source": [
    "!ls /mnt/ligandpro/db/LPCE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630ff7ca2c0945d4bfe56a975abeb095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning top-level:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary (unique per priority) ===\n",
      "final → separated → bioml → processed → raw\n",
      "\n",
      "Каждая запись засчитывается ровно в первой папке по приоритету\n",
      "\n",
      "dataset  total  final  separated  bioml  processed  raw  not_found\n",
      "  tests    834    785          0     16          0    0         33\n",
      "   moad  17832  17391          0    420          0   19          2\n",
      "pdbbind  19440  18060          0   1350          0   19         11\n",
      "\n",
      "=== Breakdown of not_found reasons ===\n",
      "dataset      reason  count\n",
      "   moad not_in_json      2\n",
      "pdbbind not_in_json     11\n",
      "  tests not_in_json     33\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "base_lpce = Path(\"/mnt/ligandpro/db/LPCE\")\n",
    "priority  = [\"final\", \"separated\", \"bioml\", \"processed\", \"raw\"]\n",
    "root_data = Path(\"/mnt/ligandpro/data/marina/sergei\")\n",
    "json_path = Path(\"/home/nikolenko/work/Projects/LPCE/data/removed_files.json\")\n",
    "\n",
    "tests_df   = pd.read_table(root_data / \"tests.tsv\")\n",
    "moad_df    = pd.read_table(root_data / \"moad.tsv\")\n",
    "pdbbind_df = pd.read_table(root_data / \"pdbbind.tsv\")\n",
    "\n",
    "def extract_ids(df: pd.DataFrame) -> set[str]:\n",
    "    for col in (\"pdb\", \"pdbid\", \"pdb_id\", \"PDB\", \"PDBID\"):\n",
    "        if col in df.columns:\n",
    "            return {str(x).strip().lower()[:4] for x in df[col]}\n",
    "    return {str(x).strip().lower()[:4] for x in df.index}\n",
    "\n",
    "datasets = {\n",
    "    \"tests\":   extract_ids(tests_df),\n",
    "    \"moad\":    extract_ids(moad_df),\n",
    "    \"pdbbind\": extract_ids(pdbbind_df),\n",
    "}\n",
    "\n",
    "found = {}\n",
    "\n",
    "def grab_codes(pattern: str) -> set[str]:\n",
    "    root = base_lpce / \"raw\" / \"pdb\"\n",
    "    return {f.name[3:7].lower() for f in root.rglob(pattern)}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as pool:\n",
    "    codes_raw = set().union(*pool.map(grab_codes, [\"pdb????.ent.gz\", \"pdb????.cif.gz\"]))\n",
    "found[\"raw\"] = codes_raw\n",
    "\n",
    "for d in tqdm(priority[:-1], desc=\"Scanning top-level\"):  # без raw\n",
    "    p = base_lpce / d\n",
    "    found[d] = {\n",
    "        item.name.split(\"_\", 1)[0].split(\".\", 1)[0].lower()[:4]\n",
    "        for item in p.iterdir()\n",
    "        if item.is_file() or item.is_dir()\n",
    "    } if p.is_dir() else set()\n",
    "\n",
    "with json_path.open() as fh:\n",
    "    data = json.load(fh)\n",
    "reason_map = {p.lower()[:4]: cat for cat, lst in data.items() for p in lst}\n",
    "\n",
    "summary_rows, reason_rows, missing_all = [], [], {}\n",
    "\n",
    "for name, ids in datasets.items():\n",
    "    assigned = set()\n",
    "    counts   = {}\n",
    "    for d in priority:\n",
    "        unique = (ids & found[d]) - assigned\n",
    "        counts[d] = len(unique)\n",
    "        assigned |= unique\n",
    "    not_found = ids - assigned\n",
    "\n",
    "    summary_rows.append(dict(dataset=name, total=len(ids), **counts, not_found=len(not_found)))\n",
    "\n",
    "    rc = Counter(reason_map.get(pid, \"not_in_json\") for pid in not_found)\n",
    "    for r, c in rc.items():\n",
    "        reason_rows.append(dict(dataset=name, reason=r, count=c))\n",
    "\n",
    "    missing_all[name] = {pid: reason_map.get(pid, \"not_in_json\") for pid in sorted(not_found)}\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(\"\\n=== Summary (unique per priority) ===\")\n",
    "print(\"final → separated → bioml → processed → raw\")\n",
    "print(\"\\nКаждая запись засчитывается ровно в первой папке по приоритету\\n\")\n",
    "print(pd.DataFrame(summary_rows).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Breakdown of not_found reasons ===\")\n",
    "print(pd.DataFrame(reason_rows).sort_values([\"dataset\", \"count\"], ascending=[True, False]).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
